{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RCVr2zDkC1qa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Find the best channel using Logistic Regression with Lasso but with a pre define value for C\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.model_selection\n",
        "import scipy.io\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.simplefilter(action = \"ignore\" ,category =FutureWarning)\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model,preprocessing\n",
        "logreg = linear_model.LogisticRegression(C=1e5,penalty='l2')\n",
        "#logreg = linear_model.LogisticRegression(C=1e5)\n",
        "files = os.listdir(\"./data\")\n",
        "acc = np.zeros(59)\n",
        "for file in files:\n",
        "  \n",
        "  index =((re.search('Ch_(.*).mat',file)).group(1))\n",
        "  mat = scipy.io.loadmat(file)\n",
        "  X = mat['Ch_Matrix']\n",
        "  y=mat['y']\n",
        "  y= np.ravel(y)\n",
        "  # Scale the data\n",
        "  X_scale = np.interp(X,(X.min(),X.max()),(-1,+1))\n",
        "  Xtr,Xts,Ytr,Yts = train_test_split(X_scale,y,test_size=0.33,shuffle=True) \n",
        "  logreg.fit(Xtr, Ytr)\n",
        "  yhat = logreg.predict(Xts)\n",
        "  acc[int(index)] = np.mean(yhat == Yts)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "js58Q__lDG52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "987d9121-3fca-413e-f744-dc5425be8838"
      },
      "cell_type": "code",
      "source": [
        "print ('Max Accuracy is {0:f}'.format(np.max(acc)))\n",
        "print ('Channel with max accuracy is : {0:d}'.format(np.argmax(acc)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Accuracy is 0.539095\n",
            "Channel with max accuracy is : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Or6OVroDH9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "b91f7573-06f4-4f23-aa9e-14ba91e3e62e"
      },
      "cell_type": "code",
      "source": [
        "# Find Optimal C for the channel with maximum Accuracy using cross validation\n",
        "\n",
        "#Find the best value of C\n",
        "from sklearn.model_selection import KFold\n",
        "npen = 20\n",
        "C_test = np.logspace(-2,2,npen)\n",
        "\n",
        "# Create the cross-validation object and error rate matrix\n",
        "nfold = 10\n",
        "kf = KFold(n_splits=nfold,shuffle=True)\n",
        "err_rate = np.zeros((npen,nfold))\n",
        "num_nonzerocoef = np.zeros((npen,nfold))\n",
        "# Create the logistic regression object\n",
        "logreg = linear_model.LogisticRegression(penalty='l1',warm_start=True)\n",
        "\n",
        "# Loop over the folds in the cross-validation\n",
        "# Get the channel file with max accuracy\n",
        "mat = scipy.io.loadmat('Ch_35.mat')\n",
        "X = mat['Ch_Matrix']\n",
        "y=mat['y']\n",
        "y= np.ravel(y)\n",
        "# Scale the data\n",
        "X_scale = np.interp(X,(X.min(),X.max()),(-1,+1))\n",
        "for ifold, Ind in enumerate(kf.split(X_scale)):        \n",
        "            \n",
        "    # Get training and test data\n",
        "  Itr, Its = Ind\n",
        "  Xtr = X_scale[Itr,:]\n",
        "  ytr = y[Itr]\n",
        "  Xts = X_scale[Its,:]\n",
        "  yts = y[Its]\n",
        "    \n",
        "    # Loop over penalty levels\n",
        "  for ipen, c in enumerate(C_test):\n",
        "        \n",
        "    # Set the penalty level      \n",
        "    logreg.C= c\n",
        "    \n",
        "    # Fit a model on the training data\n",
        "    logreg.fit(Xtr, ytr)\n",
        "    \n",
        "    # Predict the labels on the test set.\n",
        "    yhat = logreg.predict(Xts)\n",
        "        \n",
        "    # Measure the accuracy\n",
        "    err_rate[ipen,ifold] = np.mean(yhat != yts)\n",
        "    num_nonzerocoef[ipen,ifold]=np.sum(abs(logreg.coef_)>0.001)\n",
        "  print(\"Fold %d\" % ifold)\n",
        "    \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 0\n",
            "Fold 1\n",
            "Fold 2\n",
            "Fold 3\n",
            "Fold 4\n",
            "Fold 5\n",
            "Fold 6\n",
            "Fold 7\n",
            "Fold 8\n",
            "Fold 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MlOYjCZKG9qk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "5a307b5e-7609-48d1-b047-4d83a6b982a9"
      },
      "cell_type": "code",
      "source": [
        "# Plot the error rate\n",
        "import matplotlib.pyplot as plt\n",
        "err_mean = np.mean(err_rate, axis=1)\n",
        "num_nonzerocoef_mean = np.mean(num_nonzerocoef, axis=1)\n",
        "err_se = np.std(err_rate,axis=1)/np.sqrt(nfold-1)\n",
        "plt.errorbar(np.log10(C_test), err_mean, marker='o',yerr=err_se)\n",
        "plt.ylim([0.02,0.05])\n",
        "plt.grid()\n",
        "plt.xlabel('log10(C)')\n",
        "plt.ylabel('Error rate')\n",
        "\n",
        "imin = np.argmin(err_mean)\n",
        "\n",
        "print(\"The minimum test error rate = %12.4e, SE=%12.4e\" % (err_mean[imin], err_se[imin]))\n",
        "print(\"The C value corresponding to minimum error = %12.4e\" % (C_test[imin]))\n",
        "\n",
        "err_tgt = err_mean[imin] + err_se[imin]\n",
        "iopt = np.where(err_mean < err_tgt)[0][0]\n",
        "C_opt = C_test[iopt]\n",
        "\n",
        "print(\"Optimal C=%12.4e\" % C_opt)\n",
        "print(\"The test error rate = %12.4e, SE=%12.4e\" % (err_mean[iopt], err_se[iopt]))\n",
        "\n",
        "print('Accuracy =  {0:.4f}, SE={1:.4f}'.format(1-err_mean[iopt], err_se[iopt]))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The minimum test error rate =   4.6241e-01, SE=  5.2686e-03\n",
            "The C value corresponding to minimum error =   7.8476e-01\n",
            "Optimal C=  4.2813e-02\n",
            "The test error rate =   4.6692e-01, SE=  7.6256e-03\n",
            "Accuracy =  0.5331, SE=0.0076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqb2R9AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_LbaVgpmEGbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10033
        },
        "outputId": "573237c2-9797-4ea4-96c4-760f9dd7b2de"
      },
      "cell_type": "code",
      "source": [
        "# find the channel with optimal C using cross validation\n",
        "logreg = linear_model.LogisticRegression(C=C_opt,penalty='l1')\n",
        "logreg.C= C_opt\n",
        "files = os.listdir(\"./data\")\n",
        "# Number of folds\n",
        "nfold=10\n",
        "\n",
        "kf=sklearn.model_selection.KFold(n_splits=nfold,shuffle=True)\n",
        "acc=np.zeros((59,nfold))\n",
        "i =0\n",
        "for file in files:\n",
        "  index =((re.search('Ch_(.*).mat',file)).group(1))\n",
        "  mat = scipy.io.loadmat(file)\n",
        "  X = mat['Ch_Matrix']\n",
        "  \n",
        "  y=mat['y']\n",
        "  y= np.ravel(y)\n",
        "  \n",
        "  # Loop over the folds\n",
        "  for isplit,Ind in enumerate(kf.split(X)):\n",
        "    print(\"fold = %d \"%isplit)\n",
        "\t# Get the training data in the split\n",
        "    Itr,Its=Ind\n",
        "\t\n",
        "\t#Xtr,Xts,Ytr,Yts = train_test_split(X,y,test_size=0.33,shuffle=True) \n",
        "    Xtr = X[Itr]\n",
        "    Ytr = y[Itr]\n",
        "    Xts = X[Its]\n",
        "    Yts = y[Its]\n",
        "    \n",
        "    Xtr_scale = np.interp(Xtr,(Xtr.min(),Xtr.max()),(-1,+1))\n",
        "    Xts_scale = np.interp(Xts,(Xts.min(),Xts.max()),(-1,+1))\n",
        "    logreg.fit(Xtr, Ytr)\n",
        "    yhat = logreg.predict(Xts)\n",
        "    acc[int(index),isplit] =np.mean(yhat == Yts)\n",
        "\n",
        "#get the mean of accuracies over all splits\n",
        "acc_mean=np.mean(acc,axis=1)\n",
        "\n",
        "#pick the one with max accuracy\n",
        "print ('Max Accuracy is {0:f}'.format(np.max(acc_mean)))\n",
        "\n",
        "#Maximum Accuracy is for channel\n",
        "print ('Channel with max accuracy is : {0:d}'.format(np.argmax(acc_mean)))\n",
        "\n",
        "#Top 5 channels with best accuracy are :\n",
        "print(np.argsort(acc_mean)[-5:]+1)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "fold = 0 \n",
            "fold = 1 \n",
            "fold = 2 \n",
            "fold = 3 \n",
            "fold = 4 \n",
            "fold = 5 \n",
            "fold = 6 \n",
            "fold = 7 \n",
            "fold = 8 \n",
            "fold = 9 \n",
            "Max Accuracy is 0.535785\n",
            "Channel with max accuracy is : 30\n",
            "[24 48  2 11 31]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "odlZoP1rHXtX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "d3477c1e-0090-4ca3-8ade-fc1bd764e939"
      },
      "cell_type": "code",
      "source": [
        "#pick the one with max accuracy\n",
        "print ('Max Accuracy is {0:f}'.format(np.max(acc_mean)))\n",
        "\n",
        "#Maximum Accuracy is for channel\n",
        "print ('Channel with max accuracy is : {0:d}'.format(np.argmax(acc_mean)))\n",
        "\n",
        "#Top 5 channels with best accuracy are :\n",
        "print(np.argsort(acc_mean)[-5:])\n",
        "max_accuracies_ch_number = np.argsort(acc_mean)[-5:]\n",
        "max_accuracies =np.zeros(5)\n",
        "\n",
        "for i in max_accuracies_ch_number:\n",
        "  print(acc_mean[i])\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Accuracy is 0.535785\n",
            "Channel with max accuracy is : 30\n",
            "[23 47  1 10 30]\n",
            "0.5332974215327156\n",
            "0.5339802587701747\n",
            "0.5350955766922153\n",
            "0.5357707185438279\n",
            "0.5357850832640749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_aQVLAMQa5on",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c624f318-9880-4de1-daaa-da835e15b2df"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5339802587701747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKVNd0k0b3gd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}